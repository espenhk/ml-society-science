\section{More fun with latent variable models}
\only<presentation>{
  \begin{frame}
    \tableofcontents[ 
    currentsection, 
    hideothersubsections, 
    sectionstyle=show/shaded
    ] 
  \end{frame}
}

\only<article>{Clustering is the problem of automatically seggregating data of different types into clusters. When the goal is \alert{anomaly detection}, then there are typically two clusters. When the goal is \alert{compression} or \alert{auto-encoding} then there are typically as many clusters as needed for sufficienlty good accuracy.}


\begin{frame}
  \frametitle{Clusters as latent variables}
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}
      \node[RV] at (2,0) (data) {$x_t$};
      \node[RV,hidden] at (0,0) (cluster) {$c_{t}$};
      \draw[->] (cluster)--(data);
      \uncover<2>{
        \node[RV,hidden] at (0,1) (param) {$\param$};
        \draw[->] (param)--(cluster);
        \draw[->] (param)--(data);
      }
    \end{tikzpicture}
    \label{fig:bn-cluster}
    \caption{Graphical model for independent data from a cluster distribution.}
  \end{figure}
  \begin{block}{The clustering distribution}
    \only<presentation>{
      \begin{itemize}
      \item Cluster $c_t$
      \item Observation $x_t$
      \item Parameter $\param$.
      \end{itemize}
    }
    \only<article>{The learning problem is to estimate the parameter $\param$ describing the distribution of observations $x_t$ and clusters $c_t$.}
    \[
    x_t \mid c_t = c, \param \sim P_\param(x | c), \qquad c_t \mid \param \sim P_\param(c), \qquad \param \sim \bel(\param)
    \]
    \only<article>{
      Given a parameter $\param$, the clustering problem is to estimate the probability of each cluster for each new observation.
    }
    \[
    P_\param(c_t \mid x_t) = 
    \uncover<2>{\frac{P_\param(x_t \mid c_t) P_\param(c_t)}{\sum_{c'}P_\param(x_t \mid c_t = c') P_\param(c_t = c')}}
    \]
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{Bayesian formulation of the clustering problem}
  \begin{itemize}
  \item Prior $\bel$ on parameter space $\Param$.
  \item Data $x^T = x_1, \ldots, x_T$. Cluster assignments $c^T$ unknown.
  \item Posterior $\bel(\cdot \mid x^T)$.
  \end{itemize}
  \begin{block}{Posterior distribution}
    \only<article>{The data we obtain do not include the cluster assignments, but we can still formulate the posterior distribution of parameters given the data.}
    \begin{align}
      \bel(\param \mid x^T)
      &=
      \frac{P_\param(x^T) \bel(\param)}{\int_\Param P_{\param'}(x^T) \dd \bel(\param')}, &
      P_\param(x^T) &=  \sum_{c^T \in \CC^T} \overbrace{P_\param(x^T \mid c^T)}^{\mathclap{\textrm{Cluster Density}}}  \underbrace{P_\param(c^T)}_{\mathclap{\textrm{Cluster prior}}}
    \end{align}
    \only<article>{We simply need to expand the data-dependent term to include all possible cluster assignments. This is of course not trivial, since the number of assignments is exponential in $T$. However, algorithms such as Markov Chain Monte Carlo can be used instead.}
  \end{block}
  \uncover<2>{
  \begin{block}{Marginal posterior prediction}
    \[
    P_\bel(c_t \mid x_t, x^T) = 
    \int_\Param P_\param(c_t \mid x_t) \dd \bel(\param \mid x^T)
    \]
  \end{block}
}
\end{frame}

\begin{frame}
  \begin{example}[Preference clustering]
    \only<article>{The learning problem is to estimate the parameter $\param$ describing the distribution of observations $x_t$ and clusters $c_t$. In this example, we can assume}
    \[
      \CC = \{1, \ldots, C\}, \qquad x_{t.m} \in \{0,1\}.
    \]
    \only<article>{This means that all movies are either watched or not, and we'd simply want to predict which movie somebody is likely to watch. This allows us to use the following simple priors, splitting the parameters in two parts} $\param  = (\param_1, \param_2)$.
    \begin{block}{Model family}
    \begin{align}
      P_{\param_1}(c_t = c) &=  \param_{1,c},
      & c_t \sim \Multinomial(\param_1)
      \\
      P_{\param_2}(x_{t,m} = 1 \mid c_t = c)           &=\param_{2, m, c}
      & x_{t,m} \mid c_t = c \sim \Bernoulli(\param_{2,m,c})                                           
    \end{align}
  \end{block}
    \only<article>{Since everything is discrete, it makes sense that we can use a Multinomial model for the cluster distribution and a Bernoulli model for whether or not a movie was watched. Now we only need to specify a useful prior for each one of those. The standard priors to use, are a Beta prior for the Bernoulli and the Dirichlet for the Multinomial, as they are conjuate.}
    \begin{block}{Prior}
    \begin{align}
      \param_1 &\sim \Dirichlet(\vg), & \param_2 &\sim \BetaDist(\alpha, \beta)
    \end{align}
    \only<article>{Typically $\vg = (1/2, \ldots, 1/2)$ and $\alpha = \beta = 1/2$ to allow for the possibility of nearly deterministic behaviour.}
  \end{block}
  See \url{src/pymc/beta_bernoulli_clustering.py}
  \end{example}
\end{frame}

\begin{frame}
  \frametitle{Supervised learning}
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}
      \node[RV] at (0,0) (x1) {$x_1$};
      \node[RV] at (0,2) (x2) {$x_T$};
      \node[RV] at (0,4) (x3) {$x_t$};
      \node[RV] at (2,0) (y1) {$y_1$};
      \node[RV] at (2,2) (y2) {$y_T$};
      \node[RV, hidden] at (2,4) (y3) {$y_t$};
      \draw[->] (x1)--(y1);
      \draw[->] (x2)--(y2);
      \draw[->] (x3)--(y3);
    \end{tikzpicture}
    \label{fig:supervised learning}
    \caption{Graphical model for a classical supervised learning problem.}
  \end{figure}

\end{frame}


\begin{frame}
  \frametitle{Semi-supervised learning}
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}
      \node[RV] at (0,0) (x1) {$x_1$};
      \node[RV] at (0,1) (xs) {$x_i$};
      \node[RV] at (0,2) (x2) {$x_T$};
      \node[RV] at (0,4) (x3) {$x_t$};
      \node[RV] at (2,0) (y1) {$y_1$};
      \node[RV,hidden] at (2,1) (ys) {$y_i$};
      \node[RV] at (2,2) (y2) {$y_T$};
      \node[RV, hidden] at (2,4) (y3) {$y_t$};
      \draw[->] (x1)--(y1);
      \draw[->] (xs)--(ys);
      \draw[->] (x2)--(y2);
      \draw[->] (x3)--(y3);
    \end{tikzpicture}
    \label{fig:semi-supervised learning}
    \caption{Graphical model for a classical semi-supervised learning problem.}
  \end{figure}

\end{frame}

\begin{frame}
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}
      \node[RV] at (0,0) (x1) {$x_1$};
      \node[RV] at (0,1) (xs) {$x_i$};
      \node[RV] at (0,2) (x2) {$x_T$};
      \node[RV] at (0,4) (x3) {$x_t$};
      \node[RV] at (2,0) (y1) {$y_1$};
      \node[RV,hidden] at (2,1) (ys) {$y_i$};
      \node[RV] at (2,2) (y2) {$y_T$};
      \node[RV, hidden] at (2,4) (y3) {$y_t$};
      \draw[<-] (x1)--(y1);
      \draw[<-] (xs)--(ys);
      \draw[<-] (x2)--(y2);
      \draw[<-] (x3)--(y3);
    \end{tikzpicture}
    \label{fig:semi-supervised learning}
    \caption{Generative version of the semi-supervised model}
  \end{figure}

\end{frame}

\begin{frame}
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}
      \node[RV] at (0,0) (x1) {$x_1$};
      \node[RV] at (0,2) (x2) {$x_T$};
      \node[RV] at (0,4) (x3) {$x_t$};
      \node[RV, hidden] at (2,0) (y1) {$y_1$};
      \node[RV, hidden] at (2,2) (y2) {$y_T$};
      \node[RV, hidden] at (2,4) (y3) {$y_t$};
      \draw[<-] (x1)--(y1);
      \draw[<-] (x2)--(y2);
      \draw[<-] (x3)--(y3);
    \end{tikzpicture}
    \label{fig:unsupervised learning}
    \caption{Basic unsupervised learning model}
  \end{figure}
  \begin{block}{Applications}
    \begin{itemize}
    \item Clustering
    \item Compression
    \end{itemize}
  \end{block}

\end{frame}






%%% Local Variables:
%%% mode: latex
%%% TeX-engine: xetex
%%% TeX-master: "notes.tex"
%%% End:
