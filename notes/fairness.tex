\section{Fairness in machine learning}

\only<article>{
  The problem of fairness in machine learning and artificial intelligence has only recently been widely recognised. When any algorithm is implemented at scale, no matter the original objective and whether it is satisfied, it has significant societal effects. In particular, even when considering the narrow objective of the algorithm, even if it improves it overall, it may increase inequality.

  In this course we will look at two aspects of fairness. The first has to do with disadvantaged populations that form distinct social classes due to a shared income stratum, race or gender. The second has to do with meritocratic notions of fairness.
}
\begin{frame}
  \frametitle{Bail decisions}
  \only<article>{
    For our example regarding disadvantaged populations, consider the example of bail decisions in the US court system. When a defendant is charged, the judge has the option to either place them in jail pending trial, or set them free, under the condition that the defendant pays some amount of bail. The amount of bail (if any) is set to an amount that would be expected to deter flight or a relapse. 
  }

  \only<presentation>{
    \begin{columns}
      \begin{column}{0.5\textwidth}
        \centering
        \begin{tikzpicture}
          \node at (0,0) (judge) {\includegraphics[width=0.3\columnwidth]{../figures/judge}};
          \uncover<2->{
            \node at (-2,-2) (jail) {\includegraphics[width=0.3\columnwidth]{../figures/jail}};
            \draw[->] (judge) -- (jail);
          }
          \uncover<3->{
            \node at (2,-2) (bail) {\includegraphics[width=0.3\columnwidth]{../figures/bail}};
            \draw[->] (judge) -- (bail);
          }

          \uncover<4->{
            \node at (-2,-4) (trial) {\includegraphics[width=0.3\columnwidth]{../figures/trial}};
            \draw[->] (jail) -- (trial);
          }
          \uncover<5->{
            \draw[->] (bail) -- (trial);
          }
          \uncover<6->{
            \node at (2,-4) (arrest) {\includegraphics[width=0.3\columnwidth]{../figures/handcuffs}};
            \draw[->] (bail) -- (arrest);
          }
        \end{tikzpicture}
      \end{column}
      \begin{column}{0.5\textwidth}
        \centering
        \uncover<7->{
          \includegraphics[width=\textwidth]{../figures/judge-fairness}
        }
      \end{column}
    \end{columns}
  }
  
  \only<article>{
    \begin{figure}
      \centering
      \includegraphics[width=0.5\textwidth]{../figures/judge-fairness}
      \caption{In some cases, it appears as though automating this procedure might lead to better outcomes. But is that generally true?}
      \label{fig:judge-fairness}
    \end{figure}
  }

\end{frame}

\begin{frame}
  \frametitle{Whites get lower scores than blacks\footnote{Pro-publica, 2016}}
  \only<article>{In a different study, it was shown that a commonly used software tool for determining 'risk scores' in the US was biased towards white defendants, who seemed to be always getting lower scores than blacks.}
  \begin{figure}[H]
    \begin{columns}
      \begin{column}{0.5\textwidth}
        \centering
        \def\svgwidth{.95\columnwidth}
        \input{../figures/risk-scores-black.pdf_tex}
        Black
      \end{column}
      \begin{column}{0.5\textwidth}
        \centering
        \def\svgwidth{0.95\columnwidth}
        \input{../figures/risk-scores-white.pdf_tex}      
        White
      \end{column}
    \end{columns}
    \label{fig:risk-bias}
    \caption{Apparent bias in risk scores towards black versus white defendants.}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{But scores equally accurately predict recidivsm\footnote{Washington Post, 2016}}
  \only<article>{On the other hand, the scores generated by the software seemed to be very predictive on whether or not defendants would re-offend, independently of their race.}
  \begin{figure}[H]
    \centering
    \includegraphics[width=\columnwidth]{../figures/imrs}
    \caption{Recidivism rates by risk score.}
    \label{fig:imrs}
  \end{figure}
\end{frame}
\begin{frame}
  \frametitle{But non-offending blacks get higher scores}
  \only<article>{On the third hand, we see that the system seemed to give higher risk scores to non-offending blacks. So, is there a way to fix that or not?}
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{../figures/imrs-risk}
    \caption{Score breakdown based on recidivism rates.}
    \label{fig:imrs-risk}
  \end{figure}
\end{frame}

\only<presentation>{

  \begin{frame}
    \frametitle{Graphical models and independence}
    \begin{itemize}
    \item Why is it not possible to be fair in all respects?
    \item Different notions of \alert{conditional independence}.
    \item Can only be satisfied rarely simultaneously.
    \end{itemize}
  \end{frame}

}
\only<article>{
  How can we explain this discrepancy? We can show that in fact, each one of these different measures of bias in our decision rules can be seen as a notion of conditional independence. 
}
\only<presentation>{
\input{graphical-models}
}



\section{Concepts of fairness}
\begin{frame}
  \frametitle{Bail decisions, revisited}
  \only<article>{Let us think of this problem in terms of bail decisions made by a judge using some policy $\pol$ with $\pol(a \mid x)$ being the probability that the judge decides $a$ when she observes $x$. Let $y$ be the outcome, which may or may not depend on $a$. In this particular case, $a$ is either release or jail. And $y$ is appears for trial or not. If we accept the tenets of decision theory, there is also a utility function $U(a, y)$ defined on which the judge bases her decision.}
  \only<presentation>{
    \centering

    \begin{columns}
      \begin{column}{0.5\textwidth}
        \begin{tikzpicture}
          \node[label=$x$] at (-1,2) (person)
          {\includegraphics[width=0.2\columnwidth]{../figures/me-recent}};
          \node[label=$\pi$] at (0,0) (judge) {\includegraphics[width=0.3\columnwidth]{../figures/judge}};
          \draw[->] (person) -- (judge);
          \uncover<2->{
            \node[label=$a_1$] at (-2,-2) (jail) {\includegraphics[width=0.3\columnwidth]{../figures/jail}};
            \draw[->] (judge) -- (jail);
          }
          \uncover<3->{
            \node[label=$a_2$] at (2,-2) (bail) {\includegraphics[width=0.3\columnwidth]{../figures/bail}};
            \draw[->] (judge) -- (bail);
          }
          \uncover<4->{
            \node[label=$y_1$] at (-2,-4) (trial) {\includegraphics[width=0.3\columnwidth]{../figures/trial}};
            \draw[->] (jail) -- (trial);
          }
          \uncover<5->{
            \draw[->] (bail) -- (trial);
          }
          \uncover<6->{
            \node[label=$y_2$] at (2,-4) (arrest) {\includegraphics[width=0.3\columnwidth]{../figures/handcuffs}};
            \draw[->] (bail) -- (arrest);
          }
        \end{tikzpicture}
      \end{column}
      \begin{column}{0.5\textwidth}
        \uncover<2->{\[\pi(a \mid x) \tag{policy}\]}
        \uncover<4->{\[\Pr(y \mid a, x) \tag{outcome}\]}
        \uncover<7->{\[U(a,y) \tag{utility}\]}
      \end{column}
    \end{columns}
  }
  \only<article>{
    \begin{figure}[H]
      \centering
      \begin{tikzpicture}
        \node[label=$x$] at (-1,3) (person)
        {\includegraphics[width=0.1\textwidth]{../figures/me-recent}};
        \node[label=$\pi$] at (0,0) (judge) {\includegraphics[width=0.2\textwidth]{../figures/judge}};
        \draw[->] (person) -- (judge);
        \uncover<2->{
          \node[label=$a_1$] at (-2,-3) (jail) {\includegraphics[width=0.2\textwidth]{../figures/jail}};
          \draw[->] (judge) -- (jail);
        }
        \uncover<3->{
          \node[label=$a_2$] at (2,-3) (bail) {\includegraphics[width=0.2\textwidth]{../figures/bail}};
          \draw[->] (judge) -- (bail);
        }
        \uncover<4->{
          \node[label=$y_1$] at (-2,-6) (trial) {\includegraphics[width=0.2\textwidth]{../figures/trial}};
          \draw[->] (jail) -- (trial);
        }
        \uncover<5->{
          \draw[->] (bail) -- (trial);
        }
        \uncover<6->{
          \node[label=$y_2$] at (2,-6) (arrest) {\includegraphics[width=0.2\textwidth]{../figures/handcuffs}};
          \draw[->] (bail) -- (arrest);
        }
      \end{tikzpicture}
      \caption{The bail decision process, simplified.}
      \label{fig:bail-process}
    \end{figure}
  }
\end{frame}


\subsection{Group fairness and conditional independence}
\begin{frame}
  \only<article>{So how can we reframe the above fairness notions in a more precise way? Both of them involve conditional independence between $y, a$ and a sensitive attribute $z$, such as race. The first notion says that the actions of the judge (or equivalently, the scores of the algorithm) are \emph{calibrated} with respect to the outcomes. The second says that they are \emph{balanced}, so that were the outcome known to the judge, she would be making a decision independently of the defendant's race. Both of these conditions were discussed in a more restricted setting by }
  \only<presentation>{
    \only<1>{
      \includegraphics[width=\columnwidth]{../figures/imrs}
    }
    \only<2>{
      \includegraphics[width=\columnwidth]{../figures/imrs-risk}
    }
    \begin{columns}
      \begin{column}{0.3\textwidth}
        \begin{itemize}
        \item[$y$] Result.
        \item[$a$] Assigned score.
        \item[$z$] Race.
        \end{itemize}
      \end{column}
      \begin{column}{0.7\textwidth}
        \begin{align}
          \Pr^\pi(y \mid a, z) &= \Pr^\pi(y \mid a) \tag{\alert<1>{calibration}}\\
          \Pr^{\pi}(a \mid y, z) &= \Pr^{\pi}(a \mid y) \tag{\alert<2>{balance}}
        \end{align}
      \end{column}
    \end{columns}
  }
  \only<article>{
    \begin{definition}[Calibration]
      A policy $\pol$ is calibrated for parameter $\param$ with respect to $z$ if
      \begin{equation}
        \Pr_\param^\pol(y \mid a, z) = \Pr_\param^\pol(y \mid a), \qquad \forall a, z.
        \label{eq:calibration}
      \end{equation}
    \end{definition}
    You will observe that calibration here means that
    \[
    y \indep z \mid a, \param, \pol
    \]
    i.e. that $y$ is independent of $z$ given the judge's action $a$, so the distribution of outcomes is the same for every one of our actions no matter what the value of $z$ is.
    \begin{example}
      Figure~\ref{fig:imrs} shows how actual recidivism ($y$) relates to risk scores ($a$) for different races ($z$). In that case, it is apparent that, at least approximately, $P(y \mid a, z) = P(y \mid a, z')$, so the COMPAS method satisfies calibration.
    \end{example}

    \begin{definition}[Balance]
      A policy $\pol$ is balanced for parameter $\param$ with respect to $z$ if:
      \begin{equation}
        \Pr_\param^{\pol}(a \mid y, z) = \Pr_\param^{\pol}(a \mid y), \qquad \forall y, z.
        \label{eq:balance}
      \end{equation}
    \end{definition}
    On the other hand, balance means that
    \[
    a \indep z \mid y,
    \]
    i.e. that $a$ is independent of $z$ given the true outcome $y$.\footnote{This definition only really makes sense when $y$ does not depend on $a$ at all. When this is not the case, it's easy to construct a random variable $y'$ that does not depend on $a$ so that $y$ can be written as a function $y(y', a)$. Then we can achieve balance with respect to $y'$.}
    \begin{example}
      Figure~\ref{fig:imrs-risk} shows how risk scores ($a$) relate to actual recidivism ($y$) for different races ($z$). In that case, it is apparent that, at least approximately, $P(a \mid y, z) \neq P(a \mid y, z')$, so the COMPAS method does not satisfy balance.
    \end{example}
    
  }

\end{frame}


  
\subsection{Individual fairness and meritocracy.}
\only<article>{A different concept of fairness is meritocracy. For example, if one candidate for a job is better than another candidate, perhaps that candidate should be taken for the job.}


\begin{frame}

  \only<article>{Let us consider merit from the point of view of the decision maker, who can either hire $(a_t = 1)$ or not hire $(a_t = 0)$ the $t$-th applicant. If the applicant has characteristics $x_t$ and merit $y_t$, the DM's decision has utility $U(a_t, y_t)$. In order to model meritocracy, we assign an inherent \emph{quality} to $y$, expressed as an ordering, so that $U(1, y) \geq U(1, y')$ if $y \geq y'$. Assuming $P_\param(x_t, y_t)$ is known to the DM then clearly she should make the decision by solving the following maximisation problem:
  }
  \begin{block}{Meritocratic decision}
    \begin{align}
      a_t(\param, x_t) \in \argmax_a \E_\param(U \mid a, x_t)
      &=
        \int_\CY U(a_t, y) \E_\param(U \mid a_t, x_t) 
    \end{align}
  \end{block}
  \only<article>{ Here, the notion of meritocracy is defined through
    our utility function.  Although it would be better to consider the
    candidate's utility instead, this is in practice difficult,
    because we'd have to somehow estimate each individual's utility
    function.  Finally, we are taking the expectation here is because
    we may not know for certain what the quality attribute of a given
    person might be.  }
\end{frame}

\begin{frame}
  \frametitle{Smooth fairness}
  \only<article>{It makes sense to combine the idea of meritocracy with that of similarity. That is, similar people should be treated similarly. This means that we should find a policy $\pol$ that maximises utility $\util$ and makes similar decisions for similar people. 

    Let $\CX$ be equipped with a metric $\rho$, and let $D$ be a divergence between distributions, such as the KL-divergence. We can then formalise the above intuition as follows:}
  \begin{equation}
    D[\pol(a \mid x), \pol(a \mid x')]
    \leq 
    \rho(x, x').
    \label{eq:lipschitz-policies}
  \end{equation}
  \only<article>{
    This is a so-called Lipschitz condition on the policy, and is illustrated in the figure below.
  }    
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}[scale=2,thick,domain=-2:2]
      \draw[->] (-2,0) -- (2,0) node[right] {$x$};
      \draw[->] (0,0) -- (0,1) node[above] {$\pol(a \mid x)$};
      \draw[color=blue] plot (\x,{0.5 + 0.5 * tanh(\x)}); % node[right] {$f(x) = \frac{1}{20} \mathrm e^x$};
      \draw [|<->|,color=red] plot (-0.5,-0.1) -- node[below=1em] {$\rho(x,x')$} ++(1, 0);
    \end{tikzpicture}
    \caption{A Lipschitz function}
    \label{fig:lipschitz}
  \end{figure}
  \only<article>{
    If we wish to find the optimal policy that satisfies this constraint, then it is naturally to think about this as a constrained optimisation problem
    }
    \begin{block}{The constrained maximisation problem}
      \begin{equation}
        \max_\pol \cset{\util(\pol)}{\rho(x, x') \leq \epsilon}
        \label{eq:constraint-meritocracy}
    \end{equation}
  \end{block}
\only<article>{

  The practical problem in this framework is how to define the metric $\rho$ in the first place. A natural idea is to simply use the probability of $y$ as a metric. Then it doesn't matter how different $x, x'$ appear: the only important thing is how what is the distribution of $y$ implied by the different attributes. 

    \begin{block}{The metric $\rho$ in terms of the distribution $y \mid x$}
    Firstly, assume you somehow have some probability law $\Pr$ over the variables of interest. To fix ideas, let us consider the total variation norm and assume $y \in \CY$ takes a finite number of values. Then we can define the metric as
    \begin{align}
      \rho(x, x')
      &\defn \|\Pr(y \mid x) - \Pr(y \mid x')\|_1
      = \sum_{i \in \CY} \|\Pr(y = i \mid x) - \Pr(y = i \mid x')\|_1
    \end{align}
    \end{block}
  }
\end{frame}



\subsection{A unifying view of fairness}
\only<article>{In both cases, we defined conditional independence for a fixed probability distribution $P_\param(x,y,z)$ on the various variables. We also considered meritocratic fairness with respect to $\param$. First of all, we can simply define a function $F$ that measures the amount of fairness violation for a given parameter value. As an example, the deviation from calibration of a policy $\pol$ for a given parameter value $\param$ can be written as:
\begin{equation}
  F_{\textrm{calibration}}(\param, \pol)
  \defn
  \max_{a, z}
  \sum_i |\Pr_\param^\pol(y = i \mid a, z) - \Pr_\param^\pol(y = i \mid a, z')|
\end{equation}
Similarly, the deviation from calibration can be written as:
\begin{equation}
  F_{\textrm{balance}}(\param, \pol)
  \defn
  \max_{a, z}
  \sum_i |\Pr_\param^\pol(a = i \mid y, z) - \Pr_\param^\pol(a = i \mid y, z')|
\end{equation}
Finally, we can also create a deviation for the constrains in the maximisation problem of \eqref{eq:constraint-meritocracy}, so that
\begin{equation}
  F_{\textrm{merit}}(\param, \pol)
  \defn
  \exp\left(\rho(x, x') - \epsilon\right).
\end{equation}

If we the decision maker did not care about fairness, she could just maximise some utility function $\util$ of interest. In order to take fairness into account, we define the value of the policy to be a linear combination between the original utility and the fairness violation.
}

\begin{block}{The value of a policy}
  Let $\lambda$ represent the trade-off between utility and fairness.
  \only<article>{The combined value for a given parameter $\param$ is}
  \begin{equation}
    \label{eq:combined-value}
    V(\lambda, \param, \pol) = \lambda \overbrace{U(\param, \pol)}^{\textrm{utility}} - \underbrace{(1 - \lambda) F(\param, \pol)}_{\textrm{fairness violation}}
  \end{equation}
\end{block}

\only<article>{
  \begin{remark}
    It is also possible to define the above problem in terms of a constrained optimisation
    \[
      \max\cset{U(\param, \pol)}{F(\param, \pol) \leq \epsilon}.
    \]
    The advantage of specifying just a single value function to maximise is that unconstrained problems are usually simpler to handle.
\end{remark}

}
\subsection{Bayesian fairness}
\only<article>{
  The previous section defined a simple universal framework for trading off utility and fairness in terms of an unconstrained optimisation problem. This involved defining the value of a policy for a given trade-off $\lambda$ and an underlying distribution with parameter $\param$. However, in a learning context this parameter cannot be assumed to be known. One solution to this problem is to assume a Bayesian approach and simply specify a subjective probability distribution $\bel$ over parameters. Typically, this would be calculated from a prior distribution and some available data. }
\begin{frame}
  \frametitle{The Bayesian decision problem}
  \only<article>{
    In the Bayesian setting, we simply need to maximise the expected $V$ over all possible model parameters. Assume we have some probability $\bel$ over the parameters. Typically, $\bel$ would be a posterior distribution calculated from some prior and some data, but for simplicity we just write $\bel(\theta)$ in this section. Then the expected value of a policy $\pol$ is simply the following.
  }
\begin{block}{The Bayesian value of a policy}
    \only<article>{With some abuse of notation, we also define the value for a subjective belief, modelled as a distribution $\bel$, over parameters}
    \begin{equation}
      \label{eq:combined-value}
      V(\lambda, \bel, \pol) = \int_{\Param} V(\lambda, \param, \pol) \dd \bel(\param).
    \end{equation}
    \only<article>{This is simply the expected value of our policy over each possible parameter $\theta$, with respect to the belief $\bel$.}
  \end{block}
  \only<article>{
    \index{gradient ascent!stochastic}
    Maximising the value of this policy should be possible with stochastic gradient ascent we can easily obtain samples from $\bel$.
  }
\end{frame}


\subsection{Further reading}
\only<article>{ Recently algorithmic fairness has been studied quite
  extensively in the context of statistical decision
  making. \citet{dwork2012fairness,chouldechova2016fair,corbett2017algorithmic,kleinberg2016inherent,kilbertus2017avoiding}
  studied fairness under the one shot statistical decision making
  framework in this
  chapter. \citet{jabbari2016fair,joseph2016rawlsian} studied fairness
  in sequential decision making settings.  Fairness has also been
  studied in other machine learning topics, such as
  clustering~\citep{Chierichetti2017fair}, natural language
  processing~\citep{BlodgettO17} and recommendation
  systems~\citep{CelisV17}.

  Satisfying fairness constraints while maximizing expected utility in a more general setting in the conditional independence setting has been considered by~\citet{corbett2017algorithmic}, under a specific model. \citet{dimitrakakis2017:subjective-fairness} consider the Bayesian setting, where the model is unknown.
  
  In the individual fairness-as-meritocracy framework,
  \citet{dwork2012fairness} and look for decision rules that are
  smooth in a sense that similar \emph{individuals} are treated
  similarly.
  
  Finally, \citet{russell2017worlds} which considers the problem of uncertainty from the point of view of causal modelling.

}

\begin{frame}
  \begin{block}{Online resources}
    \begin{itemize}
    \item COMPAS analysis by propublica \url{https://github.com/propublica/compas-analysis}
    \item Open policing database \url{https://openpolicing.stanford.edu/}
    \end{itemize}
  \end{block}
\end{frame}


\begin{frame}
  \frametitle{Learning outcomes}
  \begin{block}{Understanding}
    \begin{itemize}
    \item Graphical models.
    \item Conditional independence.
    \item Fairness as independence.
    \item Fairness as meritocracy.
    \end{itemize}
  \end{block}
  
  \begin{block}{Skills}
    \begin{itemize}
    \item Be able to specify a graphical model capturing dependencies between variables.
    \item Be able to verify if a policy satisfies a fairness condition.
    \end{itemize}
  \end{block}

  \begin{block}{Reflection}
    \begin{itemize}
    \item When looking at sensitive attributes, how easy is it to determine fairness?
    \item How should we balance the needs of individuals, the decision maker and society?
    \item Does having more data available make it easier to achieve fairness?
    \item How do these fairness concept relate to classical game theory ideas?
    \end{itemize}
  \end{block}
  
\end{frame}





%%% Local Variables:
%%% mode: latex
%%% TeX-engine: xetex
%%% TeX-master: "notes"
%%% End:
